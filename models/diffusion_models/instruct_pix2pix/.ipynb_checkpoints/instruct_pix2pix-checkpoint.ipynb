{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f1ba210-6d13-49ee-aeda-cfcf18ab7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c476196-b519-4122-9e4d-3dc1bf6e8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"http_proxy\"] = \"http://wnxxxxxx:www\"\n",
    "os.environ[\"https_proxy\"] = \"http://wnxxxxxx:www\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564240eb-97df-45f6-a32f-386eb986830b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e369e1-2fe2-4def-8b4a-7b01b98903db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'instruct-pix2pix'...\n",
      "Updating files:  16% (5/31)\n",
      "Updating files:  19% (6/31)\n",
      "Updating files:  22% (7/31)\n",
      "Updating files:  25% (8/31)\n",
      "Updating files:  29% (9/31)\n",
      "Updating files:  32% (10/31)\n",
      "Updating files:  35% (11/31)\n",
      "Updating files:  38% (12/31)\n",
      "Updating files:  41% (13/31)\n",
      "Updating files:  45% (14/31)\n",
      "Updating files:  48% (15/31)\n",
      "Updating files:  51% (16/31)\n",
      "Updating files:  54% (17/31)\n",
      "Updating files:  58% (18/31)\n",
      "Updating files:  61% (19/31)\n",
      "Updating files:  64% (20/31)\n",
      "Updating files:  67% (21/31)\n",
      "Updating files:  70% (22/31)\n",
      "Updating files:  74% (23/31)\n",
      "Updating files:  77% (24/31)\n",
      "Updating files:  80% (25/31)\n",
      "Updating files:  83% (26/31)\n",
      "Updating files:  87% (27/31)\n",
      "Updating files:  90% (28/31)\n",
      "Updating files:  93% (29/31)\n",
      "Updating files:  96% (30/31)\n",
      "Updating files: 100% (31/31)\n",
      "Updating files: 100% (31/31), done.\n",
      "Filtering content:  11% (2/18), 2.26 GiB | 244.15 MiB/s\n",
      "Filtering content:  11% (2/18), 3.86 GiB | 18.76 MiB/s \n",
      "Filtering content:  16% (3/18), 3.86 GiB | 18.76 MiB/s\n",
      "Filtering content:  16% (3/18), 5.46 GiB | 2.21 MiB/s \n",
      "Filtering content:  22% (4/18), 5.46 GiB | 2.21 MiB/s\n",
      "Filtering content:  22% (4/18), 6.03 GiB | 5.16 MiB/s\n",
      "Filtering content:  27% (5/18), 6.03 GiB | 5.16 MiB/s\n",
      "Filtering content:  27% (5/18), 6.59 GiB | 8.31 MiB/s\n",
      "Filtering content:  33% (6/18), 6.59 GiB | 8.31 MiB/s\n",
      "Filtering content:  33% (6/18), 6.91 GiB | 6.43 MiB/s\n",
      "Filtering content:  38% (7/18), 6.91 GiB | 6.43 MiB/s\n",
      "Filtering content:  38% (7/18), 7.22 GiB | 7.52 MiB/s\n",
      "Filtering content:  44% (8/18), 7.22 GiB | 7.52 MiB/s\n",
      "Filtering content:  44% (8/18), 7.68 GiB | 8.79 MiB/s\n",
      "Filtering content:  50% (9/18), 7.68 GiB | 8.79 MiB/s\n",
      "Filtering content:  50% (9/18), 8.14 GiB | 10.32 MiB/s\n",
      "Filtering content:  55% (10/18), 8.14 GiB | 10.32 MiB/s\n",
      "Filtering content:  55% (10/18), 8.29 GiB | 6.10 MiB/s \n",
      "Filtering content:  61% (11/18), 8.29 GiB | 6.10 MiB/s\n",
      "Filtering content:  61% (11/18), 8.45 GiB | 3.04 MiB/s\n",
      "Filtering content:  66% (12/18), 8.45 GiB | 3.04 MiB/s\n",
      "Filtering content:  66% (12/18), 8.68 GiB | 16.60 MiB/s\n",
      "Filtering content:  72% (13/18), 8.68 GiB | 16.60 MiB/s\n",
      "Filtering content:  72% (13/18), 8.91 GiB | 16.79 MiB/s\n",
      "Filtering content:  77% (14/18), 8.91 GiB | 16.79 MiB/s\n",
      "Filtering content:  77% (14/18), 12.11 GiB | 4.58 MiB/s\n",
      "Filtering content:  83% (15/18), 12.11 GiB | 4.58 MiB/s\n",
      "Filtering content:  83% (15/18), 15.31 GiB | 1.66 MiB/s\n",
      "Filtering content:  88% (16/18), 15.31 GiB | 1.66 MiB/s\n",
      "Filtering content:  88% (16/18), 18.48 GiB | 6.02 MiB/s\n",
      "Filtering content:  94% (17/18), 18.48 GiB | 6.02 MiB/s\n",
      "Filtering content:  94% (17/18), 21.66 GiB | 3.34 MiB/s\n",
      "Filtering content: 100% (18/18), 21.66 GiB | 3.34 MiB/s\n",
      "Filtering content: 100% (18/18), 21.66 GiB | 17.38 MiB/s, done.\n",
      "Encountered 2 file(s) that may not have been copied correctly on Windows:\n",
      "\tinstruct-pix2pix-00-22000.safetensors\n",
      "\tinstruct-pix2pix-00-22000.ckpt\n",
      "\n",
      "See: `git lfs help smudge` for more details.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!git clone https://huggingface.co/timbrooks/instruct-pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f778a6-0d23-4b1c-9c66-d84f19eb3be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  57%|█████████████████████████████▋                      | 4/7 [00:01<00:00,  3.41it/s]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "Loading pipeline components...: 100%|████████████████████████████████████████████████████| 7/7 [00:02<00:00,  3.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [07:01<00:00, 21.10s/it]\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline\n",
    "\n",
    "model_id = \"timbrooks/instruct-pix2pix\" # <- replace this \n",
    "#pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float32).to(\"cpu\")\n",
    "#generator = torch.Generator(\"cuda\").manual_seed(0)\n",
    "generator = torch.manual_seed(0)\n",
    "\n",
    "url = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/test_pix2pix_4.png\"\n",
    "\n",
    "\n",
    "def download_image(url):\n",
    "   image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
    "   image = PIL.ImageOps.exif_transpose(image)\n",
    "   image = image.convert(\"RGB\")\n",
    "   return image\n",
    "\n",
    "image = download_image(url)\n",
    "prompt = \"wipe out the lake\"\n",
    "num_inference_steps = 20\n",
    "image_guidance_scale = 1.5\n",
    "guidance_scale = 10\n",
    "\n",
    "edited_image = pipe(prompt, \n",
    "   image=image, \n",
    "   num_inference_steps=num_inference_steps, \n",
    "   image_guidance_scale=image_guidance_scale, \n",
    "   guidance_scale=guidance_scale,\n",
    "   generator=generator,\n",
    ").images[0]\n",
    "edited_image.save(\"edited_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a5b94a-160f-4ddf-a528-10ffb3243187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n",
      "Pipelines loaded with `dtype=torch.float16` cannot run with `cpu` device. It is not recommended to move them to `cpu` as running them will fail. Please make sure to use an accelerator to run the pipeline in inference, due to the lack of support for`float16` operations on this device in PyTorch. Please, remove the `torch_dtype=torch.float16` argument, or use another device for inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StableDiffusionInstructPix2PixPipeline {\n",
       "  \"_class_name\": \"StableDiffusionInstructPix2PixPipeline\",\n",
       "  \"_diffusers_version\": \"0.23.0\",\n",
       "  \"_name_or_path\": \"timbrooks/instruct-pix2pix\",\n",
       "  \"feature_extractor\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"requires_safety_checker\": false,\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerAncestralDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64b26d-febf-4a56-a061-d66cb8b9089c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735334f-95ae-4a2a-bb90-ca2e6c1ebf7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d1070-d2e0-4db6-b635-994b1417da27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e44909-22d2-4f6f-805d-3b91ba362f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba5b04-1841-435c-94d4-05dd5b122c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8de42a5a-92d0-47f3-9fef-9831201c0a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.91it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"LayerNormKernelImpl\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m image \u001b[38;5;241m=\u001b[39m download_image(url)\n\u001b[0;32m     18\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mturn him into cyborg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages\n\u001b[0;32m     20\u001b[0m images[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion_instruct_pix2pix.py:297\u001b[0m, in \u001b[0;36mStableDiffusionInstructPix2PixPipeline.__call__\u001b[1;34m(self, prompt, image, num_inference_steps, guidance_scale, image_guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m scheduler_is_in_sigma_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msigmas\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# 2. Encode input prompt\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_images_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# 3. Preprocess image\u001b[39;00m\n\u001b[0;32m    308\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mpreprocess(image)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion_instruct_pix2pix.py:517\u001b[0m, in \u001b[0;36mStableDiffusionInstructPix2PixPipeline._encode_prompt\u001b[1;34m(self, prompt, device, num_images_per_prompt, do_classifier_free_guidance, negative_prompt, prompt_embeds, negative_prompt_embeds)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    515\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_input_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m     prompt_embeds \u001b[38;5;241m=\u001b[39m prompt_embeds[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    523\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m prompt_embeds\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_encoder\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\clip\\modeling_clip.py:822\u001b[0m, in \u001b[0;36mCLIPTextModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m    820\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\clip\\modeling_clip.py:740\u001b[0m, in \u001b[0;36mCLIPTextTransformer.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;66;03m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m _expand_mask(attention_mask, hidden_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 740\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    749\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    750\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(last_hidden_state)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\clip\\modeling_clip.py:654\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[1;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    647\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    648\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[0;32m    649\u001b[0m         hidden_states,\n\u001b[0;32m    650\u001b[0m         attention_mask,\n\u001b[0;32m    651\u001b[0m         causal_attention_mask,\n\u001b[0;32m    652\u001b[0m     )\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 654\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\models\\clip\\modeling_clip.py:382\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    380\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 382\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[0;32m    384\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[0;32m    385\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    386\u001b[0m     causal_attention_mask\u001b[38;5;241m=\u001b[39mcausal_attention_mask,\n\u001b[0;32m    387\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    388\u001b[0m )\n\u001b[0;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2513\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2514\u001b[0m     )\n\u001b[1;32m-> 2515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"LayerNormKernelImpl\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "import requests\n",
    "import torch\n",
    "from diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n",
    "\n",
    "model_id = \"timbrooks/instruct-pix2pix\"\n",
    "pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\n",
    "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg\"\n",
    "def download_image(url):\n",
    "    image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
    "    image = PIL.ImageOps.exif_transpose(image)\n",
    "    image = image.convert(\"RGB\")\n",
    "    return image\n",
    "image = download_image(url)\n",
    "\n",
    "prompt = \"turn him into cyborg\"\n",
    "images = pipe(prompt, image=image, num_inference_steps=10, image_guidance_scale=1).images\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23ede64-66a6-40c6-a0f9-dc9075547dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def add_padding(image_path, output_path, padding_size=50, padding_color=(255, 255, 255)):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Add padding\n",
    "    padded_image = cv2.copyMakeBorder(image, padding_size, padding_size, padding_size, padding_size, cv2.BORDER_CONSTANT, value=padding_color)\n",
    "\n",
    "    # Save the result\n",
    "    cv2.imwrite(output_path, padded_image)\n",
    "\n",
    "# Example usage:\n",
    "image_path = r\"C:\\Users\\wn00217454\\computer_vision\\thesis\\data\\google_dataset\\img10.jpg\"\n",
    "output_path = r\"C:\\Users\\wn00217454\\computer_vision\\thesis\\data\\google_dataset\\img_new.jpg\"\n",
    "add_padding(image_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e506c66-71d3-45ba-bd52-e3fa604dbf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on StableDiffusionInstructPix2PixPipeline in module diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_instruct_pix2pix object:\n",
      "\n",
      "class StableDiffusionInstructPix2PixPipeline(diffusers.pipelines.pipeline_utils.DiffusionPipeline, diffusers.loaders.TextualInversionLoaderMixin, diffusers.loaders.LoraLoaderMixin)\n",
      " |  StableDiffusionInstructPix2PixPipeline(vae: diffusers.models.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unet_2d_condition.UNet2DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers, safety_checker: diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker, feature_extractor: transformers.models.clip.image_processing_clip.CLIPImageProcessor, requires_safety_checker: bool = True)\n",
      " |  \n",
      " |  Pipeline for pixel-level image editing by following text instructions (based on Stable Diffusion).\n",
      " |  \n",
      " |  This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
      " |  implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
      " |  \n",
      " |  The pipeline also inherits the following loading methods:\n",
      " |      - [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`] for loading textual inversion embeddings\n",
      " |      - [`~loaders.LoraLoaderMixin.load_lora_weights`] for loading LoRA weights\n",
      " |      - [`~loaders.LoraLoaderMixin.save_lora_weights`] for saving LoRA weights\n",
      " |  \n",
      " |  Args:\n",
      " |      vae ([`AutoencoderKL`]):\n",
      " |          Variational Auto-Encoder (VAE) model to encode and decode images to and from latent representations.\n",
      " |      text_encoder ([`~transformers.CLIPTextModel`]):\n",
      " |          Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).\n",
      " |      tokenizer ([`~transformers.CLIPTokenizer`]):\n",
      " |          A `CLIPTokenizer` to tokenize text.\n",
      " |      unet ([`UNet2DConditionModel`]):\n",
      " |          A `UNet2DConditionModel` to denoise the encoded image latents.\n",
      " |      scheduler ([`SchedulerMixin`]):\n",
      " |          A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n",
      " |          [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
      " |      safety_checker ([`StableDiffusionSafetyChecker`]):\n",
      " |          Classification module that estimates whether generated images could be considered offensive or harmful.\n",
      " |          Please refer to the [model card](https://huggingface.co/runwayml/stable-diffusion-v1-5) for more details\n",
      " |          about a model's potential harms.\n",
      " |      feature_extractor ([`~transformers.CLIPImageProcessor`]):\n",
      " |          A `CLIPImageProcessor` to extract features from generated images; used as inputs to the `safety_checker`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StableDiffusionInstructPix2PixPipeline\n",
      " |      diffusers.pipelines.pipeline_utils.DiffusionPipeline\n",
      " |      diffusers.configuration_utils.ConfigMixin\n",
      " |      diffusers.utils.hub_utils.PushToHubMixin\n",
      " |      diffusers.loaders.TextualInversionLoaderMixin\n",
      " |      diffusers.loaders.LoraLoaderMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, prompt: Union[str, List[str]] = None, image: Union[PIL.Image.Image, numpy.ndarray, torch.FloatTensor, List[PIL.Image.Image], List[numpy.ndarray], List[torch.FloatTensor]] = None, num_inference_steps: int = 100, guidance_scale: float = 7.5, image_guidance_scale: float = 1.5, negative_prompt: Union[List[str], str, NoneType] = None, num_images_per_prompt: Optional[int] = 1, eta: float = 0.0, generator: Union[torch._C.Generator, List[torch._C.Generator], NoneType] = None, latents: Optional[torch.FloatTensor] = None, prompt_embeds: Optional[torch.FloatTensor] = None, negative_prompt_embeds: Optional[torch.FloatTensor] = None, output_type: Optional[str] = 'pil', return_dict: bool = True, callback_on_step_end: Optional[Callable[[int, int, Dict], NoneType]] = None, callback_on_step_end_tensor_inputs: List[str] = ['latents'], **kwargs)\n",
      " |      The call function to the pipeline for generation.\n",
      " |      \n",
      " |      Args:\n",
      " |          prompt (`str` or `List[str]`, *optional*):\n",
      " |              The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`.\n",
      " |          image (`torch.FloatTensor` `np.ndarray`, `PIL.Image.Image`, `List[torch.FloatTensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n",
      " |              `Image` or tensor representing an image batch to be repainted according to `prompt`. Can also accept\n",
      " |              image latents as `image`, but if passing latents directly it is not encoded again.\n",
      " |          num_inference_steps (`int`, *optional*, defaults to 100):\n",
      " |              The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
      " |              expense of slower inference.\n",
      " |          guidance_scale (`float`, *optional*, defaults to 7.5):\n",
      " |              A higher guidance scale value encourages the model to generate images closely linked to the text\n",
      " |              `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.\n",
      " |          image_guidance_scale (`float`, *optional*, defaults to 1.5):\n",
      " |              Push the generated image towards the inital `image`. Image guidance scale is enabled by setting\n",
      " |              `image_guidance_scale > 1`. Higher image guidance scale encourages generated images that are closely\n",
      " |              linked to the source `image`, usually at the expense of lower image quality. This pipeline requires a\n",
      " |              value of at least `1`.\n",
      " |          negative_prompt (`str` or `List[str]`, *optional*):\n",
      " |              The prompt or prompts to guide what to not include in image generation. If not defined, you need to\n",
      " |              pass `negative_prompt_embeds` instead. Ignored when not using guidance (`guidance_scale < 1`).\n",
      " |          num_images_per_prompt (`int`, *optional*, defaults to 1):\n",
      " |              The number of images to generate per prompt.\n",
      " |          eta (`float`, *optional*, defaults to 0.0):\n",
      " |              Corresponds to parameter eta (η) from the [DDIM](https://arxiv.org/abs/2010.02502) paper. Only applies\n",
      " |              to the [`~schedulers.DDIMScheduler`], and is ignored in other schedulers.\n",
      " |          generator (`torch.Generator`, *optional*):\n",
      " |              A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
      " |              generation deterministic.\n",
      " |          latents (`torch.FloatTensor`, *optional*):\n",
      " |              Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n",
      " |              generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
      " |              tensor is generated by sampling using the supplied random `generator`.\n",
      " |          prompt_embeds (`torch.FloatTensor`, *optional*):\n",
      " |              Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n",
      " |              provided, text embeddings are generated from the `prompt` input argument.\n",
      " |          negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
      " |              Pre-generated negative text embeddings. Can be used to easily tweak text inputs (prompt weighting). If\n",
      " |              not provided, `negative_prompt_embeds` are generated from the `negative_prompt` input argument.\n",
      " |          output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
      " |              The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n",
      " |          return_dict (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
      " |              plain tuple.\n",
      " |          callback_on_step_end (`Callable`, *optional*):\n",
      " |              A function that calls at the end of each denoising steps during the inference. The function is called\n",
      " |              with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n",
      " |              callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n",
      " |              `callback_on_step_end_tensor_inputs`.\n",
      " |          callback_on_step_end_tensor_inputs (`List`, *optional*):\n",
      " |              The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n",
      " |              will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n",
      " |              `._callback_tensor_inputs` attribute of your pipeine class.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> import PIL\n",
      " |      >>> import requests\n",
      " |      >>> import torch\n",
      " |      >>> from io import BytesIO\n",
      " |      \n",
      " |      >>> from diffusers import StableDiffusionInstructPix2PixPipeline\n",
      " |      \n",
      " |      \n",
      " |      >>> def download_image(url):\n",
      " |      ...     response = requests.get(url)\n",
      " |      ...     return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
      " |      \n",
      " |      \n",
      " |      >>> img_url = \"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/mountain.png\"\n",
      " |      \n",
      " |      >>> image = download_image(img_url).resize((512, 512))\n",
      " |      \n",
      " |      >>> pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n",
      " |      ...     \"timbrooks/instruct-pix2pix\", torch_dtype=torch.float16\n",
      " |      ... )\n",
      " |      >>> pipe = pipe.to(\"cuda\")\n",
      " |      \n",
      " |      >>> prompt = \"make the mountains snowy\"\n",
      " |      >>> image = pipe(prompt=prompt, image=image).images[0]\n",
      " |      ```\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:\n",
      " |              If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] is returned,\n",
      " |              otherwise a `tuple` is returned where the first element is a list with the generated images and the\n",
      " |              second element is a list of `bool`s indicating whether the corresponding generated image contains\n",
      " |              \"not-safe-for-work\" (nsfw) content.\n",
      " |  \n",
      " |  __init__(self, vae: diffusers.models.autoencoder_kl.AutoencoderKL, text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel, tokenizer: transformers.models.clip.tokenization_clip.CLIPTokenizer, unet: diffusers.models.unet_2d_condition.UNet2DConditionModel, scheduler: diffusers.schedulers.scheduling_utils.KarrasDiffusionSchedulers, safety_checker: diffusers.pipelines.stable_diffusion.safety_checker.StableDiffusionSafetyChecker, feature_extractor: transformers.models.clip.image_processing_clip.CLIPImageProcessor, requires_safety_checker: bool = True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  check_inputs(self, prompt, callback_steps, negative_prompt=None, prompt_embeds=None, negative_prompt_embeds=None, callback_on_step_end_tensor_inputs=None)\n",
      " |  \n",
      " |  decode_latents(self, latents)\n",
      " |      # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.decode_latents\n",
      " |  \n",
      " |  disable_freeu(self)\n",
      " |      Disables the FreeU mechanism if enabled.\n",
      " |  \n",
      " |  enable_freeu(self, s1: float, s2: float, b1: float, b2: float)\n",
      " |      Enables the FreeU mechanism as in https://arxiv.org/abs/2309.11497.\n",
      " |      \n",
      " |      The suffixes after the scaling factors represent the stages where they are being applied.\n",
      " |      \n",
      " |      Please refer to the [official repository](https://github.com/ChenyangSi/FreeU) for combinations of the values\n",
      " |      that are known to work well for different pipelines such as Stable Diffusion v1, v2, and Stable Diffusion XL.\n",
      " |      \n",
      " |      Args:\n",
      " |          s1 (`float`):\n",
      " |              Scaling factor for stage 1 to attenuate the contributions of the skip features. This is done to\n",
      " |              mitigate \"oversmoothing effect\" in the enhanced denoising process.\n",
      " |          s2 (`float`):\n",
      " |              Scaling factor for stage 2 to attenuate the contributions of the skip features. This is done to\n",
      " |              mitigate \"oversmoothing effect\" in the enhanced denoising process.\n",
      " |          b1 (`float`): Scaling factor for stage 1 to amplify the contributions of backbone features.\n",
      " |          b2 (`float`): Scaling factor for stage 2 to amplify the contributions of backbone features.\n",
      " |  \n",
      " |  prepare_extra_step_kwargs(self, generator, eta)\n",
      " |      # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs\n",
      " |  \n",
      " |  prepare_image_latents(self, image, batch_size, num_images_per_prompt, dtype, device, do_classifier_free_guidance, generator=None)\n",
      " |  \n",
      " |  prepare_latents(self, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None)\n",
      " |      # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_latents\n",
      " |  \n",
      " |  run_safety_checker(self, image, device, dtype)\n",
      " |      # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.run_safety_checker\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  do_classifier_free_guidance\n",
      " |  \n",
      " |  guidance_scale\n",
      " |  \n",
      " |  image_guidance_scale\n",
      " |  \n",
      " |  num_timesteps\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  model_cpu_offload_seq = 'text_encoder->unet->vae'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Any)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  disable_attention_slicing(self)\n",
      " |      Disable sliced attention computation. If `enable_attention_slicing` was previously called, attention is\n",
      " |      computed in one step.\n",
      " |  \n",
      " |  disable_xformers_memory_efficient_attention(self)\n",
      " |      Disable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/).\n",
      " |  \n",
      " |  enable_attention_slicing(self, slice_size: Union[str, int, NoneType] = 'auto')\n",
      " |      Enable sliced attention computation. When this option is enabled, the attention module splits the input tensor\n",
      " |      in slices to compute attention in several steps. For more than one attention head, the computation is performed\n",
      " |      sequentially over each head. This is useful to save some memory in exchange for a small speed decrease.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      ⚠️ Don't enable attention slicing if you're already using `scaled_dot_product_attention` (SDPA) from PyTorch\n",
      " |      2.0 or xFormers. These attention computations are already very memory efficient so you won't need to enable\n",
      " |      this function. If you enable attention slicing with SDPA or xFormers, it can lead to serious slow downs!\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          slice_size (`str` or `int`, *optional*, defaults to `\"auto\"`):\n",
      " |              When `\"auto\"`, halves the input to the attention heads, so attention will be computed in two steps. If\n",
      " |              `\"max\"`, maximum amount of memory will be saved by running only one slice at a time. If a number is\n",
      " |              provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`\n",
      " |              must be a multiple of `slice_size`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> import torch\n",
      " |      >>> from diffusers import StableDiffusionPipeline\n",
      " |      \n",
      " |      >>> pipe = StableDiffusionPipeline.from_pretrained(\n",
      " |      ...     \"runwayml/stable-diffusion-v1-5\",\n",
      " |      ...     torch_dtype=torch.float16,\n",
      " |      ...     use_safetensors=True,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> prompt = \"a photo of an astronaut riding a horse on mars\"\n",
      " |      >>> pipe.enable_attention_slicing()\n",
      " |      >>> image = pipe(prompt).images[0]\n",
      " |      ```\n",
      " |  \n",
      " |  enable_model_cpu_offload(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = 'cuda')\n",
      " |      Offloads all models to CPU using accelerate, reducing memory usage with a low impact on performance. Compared\n",
      " |      to `enable_sequential_cpu_offload`, this method moves one whole model at a time to the GPU when its `forward`\n",
      " |      method is called, and the model remains in GPU until the next model runs. Memory savings are lower than with\n",
      " |      `enable_sequential_cpu_offload`, but performance is much better due to the iterative execution of the `unet`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          gpu_id (`int`, *optional*):\n",
      " |              The ID of the accelerator that shall be used in inference. If not specified, it will default to 0.\n",
      " |          device (`torch.Device` or `str`, *optional*, defaults to \"cuda\"):\n",
      " |              The PyTorch device type of the accelerator that shall be used in inference. If not specified, it will\n",
      " |              default to \"cuda\".\n",
      " |  \n",
      " |  enable_sequential_cpu_offload(self, gpu_id: Optional[int] = None, device: Union[torch.device, str] = 'cuda')\n",
      " |      Offloads all models to CPU using 🤗 Accelerate, significantly reducing memory usage. When called, the state\n",
      " |      dicts of all `torch.nn.Module` components (except those in `self._exclude_from_cpu_offload`) are saved to CPU\n",
      " |      and then moved to `torch.device('meta')` and loaded to GPU only when their specific submodule has its `forward`\n",
      " |      method called. Offloading happens on a submodule basis. Memory savings are higher than with\n",
      " |      `enable_model_cpu_offload`, but performance is lower.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          gpu_id (`int`, *optional*):\n",
      " |              The ID of the accelerator that shall be used in inference. If not specified, it will default to 0.\n",
      " |          device (`torch.Device` or `str`, *optional*, defaults to \"cuda\"):\n",
      " |              The PyTorch device type of the accelerator that shall be used in inference. If not specified, it will\n",
      " |              default to \"cuda\".\n",
      " |  \n",
      " |  enable_xformers_memory_efficient_attention(self, attention_op: Optional[Callable] = None)\n",
      " |      Enable memory efficient attention from [xFormers](https://facebookresearch.github.io/xformers/). When this\n",
      " |      option is enabled, you should observe lower GPU memory usage and a potential speed up during inference. Speed\n",
      " |      up during training is not guaranteed.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      ⚠️ When memory efficient attention and sliced attention are both enabled, memory efficient attention takes\n",
      " |      precedent.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Parameters:\n",
      " |          attention_op (`Callable`, *optional*):\n",
      " |              Override the default `None` operator for use as `op` argument to the\n",
      " |              [`memory_efficient_attention()`](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention)\n",
      " |              function of xFormers.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> import torch\n",
      " |      >>> from diffusers import DiffusionPipeline\n",
      " |      >>> from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n",
      " |      \n",
      " |      >>> pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n",
      " |      >>> pipe = pipe.to(\"cuda\")\n",
      " |      >>> pipe.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
      " |      >>> # Workaround for not accepting attention shape using VAE for Flash Attention\n",
      " |      >>> pipe.vae.enable_xformers_memory_efficient_attention(attention_op=None)\n",
      " |      ```\n",
      " |  \n",
      " |  maybe_free_model_hooks(self)\n",
      " |      Function that offloads all components, removes all model hooks that were added when using\n",
      " |      `enable_model_cpu_offload` and then applies them again. In case the model has not been offloaded this function\n",
      " |      is a no-op. Make sure to add this function to the end of the `__call__` function of your pipeline so that it\n",
      " |      functions correctly when applying enable_model_cpu_offload.\n",
      " |  \n",
      " |  progress_bar(self, iterable=None, total=None)\n",
      " |  \n",
      " |  register_modules(self, **kwargs)\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, variant: Optional[str] = None, push_to_hub: bool = False, **kwargs)\n",
      " |      Save all saveable variables of the pipeline to a directory. A pipeline variable can be saved and loaded if its\n",
      " |      class implements both a save and loading method. The pipeline is easily reloaded using the\n",
      " |      [`~DiffusionPipeline.from_pretrained`] class method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to save a pipeline to. Will be created if it doesn't exist.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.\n",
      " |          variant (`str`, *optional*):\n",
      " |              If specified, weights are saved in the format `pytorch_model.<variant>.bin`.\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  set_attention_slice(self, slice_size: Optional[int])\n",
      " |  \n",
      " |  set_progress_bar_config(self, **kwargs)\n",
      " |  \n",
      " |  set_use_memory_efficient_attention_xformers(self, valid: bool, attention_op: Optional[Callable] = None) -> None\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Performs Pipeline dtype and/or device conversion. A torch.dtype and torch.device are inferred from the\n",
      " |      arguments of `self.to(*args, **kwargs).`\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |          If the pipeline already has the correct torch.dtype and torch.device, then it is returned as is. Otherwise,\n",
      " |          the returned pipeline is a copy of self with the desired torch.dtype and torch.device.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      \n",
      " |      Here are the ways to call `to`:\n",
      " |      \n",
      " |      - `to(dtype, silence_dtype_warnings=False) → DiffusionPipeline` to return a pipeline with the specified\n",
      " |        [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)\n",
      " |      - `to(device, silence_dtype_warnings=False) → DiffusionPipeline` to return a pipeline with the specified\n",
      " |        [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)\n",
      " |      - `to(device=None, dtype=None, silence_dtype_warnings=False) → DiffusionPipeline` to return a pipeline with the\n",
      " |        specified [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) and\n",
      " |        [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dtype (`torch.dtype`, *optional*):\n",
      " |              Returns a pipeline with the specified\n",
      " |              [`dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)\n",
      " |          device (`torch.Device`, *optional*):\n",
      " |              Returns a pipeline with the specified\n",
      " |              [`device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)\n",
      " |          silence_dtype_warnings (`str`, *optional*, defaults to `False`):\n",
      " |              Whether to omit warnings if the target `dtype` is not compatible with the target `device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`DiffusionPipeline`]: The pipeline converted to specified `dtype` and/or `dtype`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  download(pretrained_model_name, **kwargs) -> Union[str, os.PathLike] from builtins.type\n",
      " |      Download and cache a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name (`str` or `os.PathLike`, *optional*):\n",
      " |              A string, the *repository id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
      " |              hosted on the Hub.\n",
      " |          custom_pipeline (`str`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repository id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained\n",
      " |                    pipeline hosted on the Hub. The repository must contain a file called `pipeline.py` that defines\n",
      " |                    the custom pipeline.\n",
      " |      \n",
      " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
      " |                    [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
      " |                    names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
      " |                    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
      " |                    current `main` branch of GitHub.\n",
      " |      \n",
      " |                  - A path to a *directory* (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
      " |                    must contain a file called `pipeline.py` that defines the custom pipeline.\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              🧪 This is an experimental feature and may change in the future.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |              For more information on how to load and create custom pipelines, take a look at [How to contribute a\n",
      " |              community pipeline](https://huggingface.co/docs/diffusers/main/en/using-diffusers/contribute_pipeline).\n",
      " |      \n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          custom_revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
      " |              `revision` when loading a custom pipeline from the Hub. It can be a 🤗 Diffusers version when loading a\n",
      " |              custom pipeline from GitHub, otherwise it defaults to `\"main\"` when loading from the Hub.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |          variant (`str`, *optional*):\n",
      " |              Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
      " |              loading `from_flax`.\n",
      " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
      " |              If set to `None`, the safetensors weights are downloaded if they're available **and** if the\n",
      " |              safetensors library is installed. If set to `True`, the model is forcibly loaded from safetensors\n",
      " |              weights. If set to `False`, safetensors weights are not loaded.\n",
      " |          use_onnx (`bool`, *optional*, defaults to `False`):\n",
      " |              If set to `True`, ONNX weights will always be downloaded if present. If set to `False`, ONNX weights\n",
      " |              will never be downloaded. By default `use_onnx` defaults to the `_is_onnx` class attribute which is\n",
      " |              `False` for non-ONNX pipelines and `True` for ONNX pipelines. ONNX weights include both files ending\n",
      " |              with `.onnx` and `.pb`.\n",
      " |          trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to allow for custom pipelines and components defined on the Hub in their own files. This\n",
      " |              option should only be set to `True` for repositories you trust and in which you have read the code, as\n",
      " |              it will execute code present on the Hub on your local machine.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `os.PathLike`:\n",
      " |              A path to the downloaded pipeline.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      To use private or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models), log-in with\n",
      " |      `huggingface-cli login`.\n",
      " |      \n",
      " |      </Tip>\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike, NoneType], **kwargs) from builtins.type\n",
      " |      Instantiate a PyTorch diffusion pipeline from pretrained pipeline weights.\n",
      " |      \n",
      " |      The pipeline is set in evaluation mode (`model.eval()`) by default.\n",
      " |      \n",
      " |      If you get the error message below, you need to finetune the weights for your downstream task:\n",
      " |      \n",
      " |      ```\n",
      " |      Some weights of UNet2DConditionModel were not initialized from the model checkpoint at runwayml/stable-diffusion-v1-5 and are newly initialized because the shapes did not match:\n",
      " |      - conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 9, 3, 3]) in the model instantiated\n",
      " |      You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " |      ```\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repo id* (for example `CompVis/ldm-text2im-large-256`) of a pretrained pipeline\n",
      " |                    hosted on the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_pipeline_directory/`) containing pipeline weights\n",
      " |                    saved using\n",
      " |                  [`~DiffusionPipeline.save_pretrained`].\n",
      " |          torch_dtype (`str` or `torch.dtype`, *optional*):\n",
      " |              Override the default `torch.dtype` and load the model with another dtype. If \"auto\" is passed, the\n",
      " |              dtype is automatically derived from the model's weights.\n",
      " |          custom_pipeline (`str`, *optional*):\n",
      " |      \n",
      " |              <Tip warning={true}>\n",
      " |      \n",
      " |              🧪 This is an experimental feature and may change in the future.\n",
      " |      \n",
      " |              </Tip>\n",
      " |      \n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *repo id* (for example `hf-internal-testing/diffusers-dummy-pipeline`) of a custom\n",
      " |                    pipeline hosted on the Hub. The repository must contain a file called pipeline.py that defines\n",
      " |                    the custom pipeline.\n",
      " |                  - A string, the *file name* of a community pipeline hosted on GitHub under\n",
      " |                    [Community](https://github.com/huggingface/diffusers/tree/main/examples/community). Valid file\n",
      " |                    names must match the file name and not the pipeline script (`clip_guided_stable_diffusion`\n",
      " |                    instead of `clip_guided_stable_diffusion.py`). Community pipelines are always loaded from the\n",
      " |                    current main branch of GitHub.\n",
      " |                  - A path to a directory (`./my_pipeline_directory/`) containing a custom pipeline. The directory\n",
      " |                    must contain a file called `pipeline.py` that defines the custom pipeline.\n",
      " |      \n",
      " |              For more information on how to load and create custom pipelines, please have a look at [Loading and\n",
      " |              Adding Custom\n",
      " |              Pipelines](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview)\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          custom_revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id similar to\n",
      " |              `revision` when loading a custom pipeline from the Hub. It can be a 🤗 Diffusers version when loading a\n",
      " |              custom pipeline from GitHub, otherwise it defaults to `\"main\"` when loading from the Hub.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you’re downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |          device_map (`str` or `Dict[str, Union[int, str, torch.device]]`, *optional*):\n",
      " |              A map that specifies where each submodule should go. It doesn’t need to be defined for each\n",
      " |              parameter/buffer name; once a given module name is inside, every submodule of it will be sent to the\n",
      " |              same device.\n",
      " |      \n",
      " |              Set `device_map=\"auto\"` to have 🤗 Accelerate automatically compute the most optimized `device_map`. For\n",
      " |              more information about each option see [designing a device\n",
      " |              map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
      " |          max_memory (`Dict`, *optional*):\n",
      " |              A dictionary device identifier for the maximum memory. Will default to the maximum memory available for\n",
      " |              each GPU and the available CPU RAM if unset.\n",
      " |          offload_folder (`str` or `os.PathLike`, *optional*):\n",
      " |              The path to offload weights if device_map contains the value `\"disk\"`.\n",
      " |          offload_state_dict (`bool`, *optional*):\n",
      " |              If `True`, temporarily offloads the CPU state dict to the hard drive to avoid running out of CPU RAM if\n",
      " |              the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to `True`\n",
      " |              when there is some disk offload.\n",
      " |          low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
      " |              Speed up model loading only loading the pretrained weights and not initializing the weights. This also\n",
      " |              tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      " |              Only supported for PyTorch >= 1.9.0. If you are using an older version of PyTorch, setting this\n",
      " |              argument to `True` will raise an error.\n",
      " |          use_safetensors (`bool`, *optional*, defaults to `None`):\n",
      " |              If set to `None`, the safetensors weights are downloaded if they're available **and** if the\n",
      " |              safetensors library is installed. If set to `True`, the model is forcibly loaded from safetensors\n",
      " |              weights. If set to `False`, safetensors weights are not loaded.\n",
      " |          use_onnx (`bool`, *optional*, defaults to `None`):\n",
      " |              If set to `True`, ONNX weights will always be downloaded if present. If set to `False`, ONNX weights\n",
      " |              will never be downloaded. By default `use_onnx` defaults to the `_is_onnx` class attribute which is\n",
      " |              `False` for non-ONNX pipelines and `True` for ONNX pipelines. ONNX weights include both files ending\n",
      " |              with `.onnx` and `.pb`.\n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to overwrite load and saveable variables (the pipeline components of the specific pipeline\n",
      " |              class). The overwritten components are passed directly to the pipelines `__init__` method. See example\n",
      " |              below for more information.\n",
      " |          variant (`str`, *optional*):\n",
      " |              Load weights from a specified variant filename such as `\"fp16\"` or `\"ema\"`. This is ignored when\n",
      " |              loading `from_flax`.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      To use private or [gated](https://huggingface.co/docs/hub/models-gated#gated-models) models, log-in with\n",
      " |      `huggingface-cli login`.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from diffusers import DiffusionPipeline\n",
      " |      \n",
      " |      >>> # Download pipeline from huggingface.co and cache.\n",
      " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\")\n",
      " |      \n",
      " |      >>> # Download pipeline that requires an authorization token\n",
      " |      >>> # For more information on access tokens, please refer to this section\n",
      " |      >>> # of the documentation](https://huggingface.co/docs/hub/security-tokens)\n",
      " |      >>> pipeline = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
      " |      \n",
      " |      >>> # Use a different scheduler\n",
      " |      >>> from diffusers import LMSDiscreteScheduler\n",
      " |      \n",
      " |      >>> scheduler = LMSDiscreteScheduler.from_config(pipeline.scheduler.config)\n",
      " |      >>> pipeline.scheduler = scheduler\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  numpy_to_pil(images)\n",
      " |      Convert a NumPy image or a batch of images to a PIL image.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  components\n",
      " |      The `self.components` property can be useful to run different pipelines with the same weights and\n",
      " |      configurations without reallocating additional memory.\n",
      " |      \n",
      " |      Returns (`dict`):\n",
      " |          A dictionary containing all the modules needed to initialize the pipeline.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from diffusers import (\n",
      " |      ...     StableDiffusionPipeline,\n",
      " |      ...     StableDiffusionImg2ImgPipeline,\n",
      " |      ...     StableDiffusionInpaintPipeline,\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> text2img = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
      " |      >>> img2img = StableDiffusionImg2ImgPipeline(**text2img.components)\n",
      " |      >>> inpaint = StableDiffusionInpaintPipeline(**text2img.components)\n",
      " |      ```\n",
      " |  \n",
      " |  device\n",
      " |      Returns:\n",
      " |          `torch.device`: The torch device on which the pipeline is located.\n",
      " |  \n",
      " |  dtype\n",
      " |      Returns:\n",
      " |          `torch.dtype`: The torch dtype on which the pipeline is located.\n",
      " |  \n",
      " |  name_or_path\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.pipelines.pipeline_utils.DiffusionPipeline:\n",
      " |  \n",
      " |  config_name = 'model_index.json'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Any\n",
      " |      The only reason we overwrite `getattr` here is to gracefully deprecate accessing\n",
      " |      config attributes directly. See https://github.com/huggingface/diffusers/pull/3129\n",
      " |      \n",
      " |      Tihs funtion is mostly copied from PyTorch's __getattr__ overwrite:\n",
      " |      https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  register_to_config(self, **kwargs)\n",
      " |  \n",
      " |  save_config(self, save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)\n",
      " |      Save a configuration object to the directory specified in `save_directory` so that it can be reloaded using the\n",
      " |      [`~ConfigMixin.from_config`] class method.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory where the configuration JSON file is saved (will be created if it does not exist).\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face Hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |  \n",
      " |  to_json_file(self, json_file_path: Union[str, os.PathLike])\n",
      " |      Save the configuration instance's parameters to a JSON file.\n",
      " |      \n",
      " |      Args:\n",
      " |          json_file_path (`str` or `os.PathLike`):\n",
      " |              Path to the JSON file to save a configuration instance's parameters.\n",
      " |  \n",
      " |  to_json_string(self) -> str\n",
      " |      Serializes the configuration instance to a JSON string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`:\n",
      " |              String containing all the attributes that make up the configuration instance in JSON format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  extract_init_dict(config_dict, **kwargs) from builtins.type\n",
      " |  \n",
      " |  from_config(config: Union[diffusers.configuration_utils.FrozenDict, Dict[str, Any]] = None, return_unused_kwargs=False, **kwargs) from builtins.type\n",
      " |      Instantiate a Python class from a config dictionary.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          config (`Dict[str, Any]`):\n",
      " |              A config dictionary from which the Python class is instantiated. Make sure to only load configuration\n",
      " |              files of compatible classes.\n",
      " |          return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether kwargs that are not consumed by the Python class should be returned or not.\n",
      " |          kwargs (remaining dictionary of keyword arguments, *optional*):\n",
      " |              Can be used to update the configuration object (after it is loaded) and initiate the Python class.\n",
      " |              `**kwargs` are passed directly to the underlying scheduler/model's `__init__` method and eventually\n",
      " |              overwrite the same named arguments in `config`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`ModelMixin`] or [`SchedulerMixin`]:\n",
      " |              A model or scheduler object instantiated from a config dictionary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> from diffusers import DDPMScheduler, DDIMScheduler, PNDMScheduler\n",
      " |      \n",
      " |      >>> # Download scheduler from huggingface.co and cache.\n",
      " |      >>> scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cifar10-32\")\n",
      " |      \n",
      " |      >>> # Instantiate DDIM scheduler class with same config as DDPM\n",
      " |      >>> scheduler = DDIMScheduler.from_config(scheduler.config)\n",
      " |      \n",
      " |      >>> # Instantiate PNDM scheduler class with same config as DDPM\n",
      " |      >>> scheduler = PNDMScheduler.from_config(scheduler.config)\n",
      " |      ```\n",
      " |  \n",
      " |  get_config_dict(*args, **kwargs) from builtins.type\n",
      " |  \n",
      " |  load_config(pretrained_model_name_or_path: Union[str, os.PathLike], return_unused_kwargs=False, return_commit_hash=False, **kwargs) -> Tuple[Dict[str, Any], Dict[str, Any]] from builtins.type\n",
      " |      Load a model or scheduler configuration.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
      " |                    the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_model_directory`) containing model weights saved with\n",
      " |                    [`~ConfigMixin.save_config`].\n",
      " |      \n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          return_unused_kwargs (`bool`, *optional*, defaults to `False):\n",
      " |              Whether unused keyword arguments of the config are returned.\n",
      " |          return_commit_hash (`bool`, *optional*, defaults to `False):\n",
      " |              Whether the `commit_hash` of the loaded configuration are returned.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `dict`:\n",
      " |              A dictionary of all the parameters stored in a JSON configuration file.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  config\n",
      " |      Returns the config of the class as a frozen dictionary\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, Any]`: Config of the class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.configuration_utils.ConfigMixin:\n",
      " |  \n",
      " |  has_compatibles = False\n",
      " |  \n",
      " |  ignore_for_config = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.utils.hub_utils.PushToHubMixin:\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Optional[str] = None, create_pr: bool = False, safe_serialization: bool = True, variant: Optional[str] = None) -> str\n",
      " |      Upload model, scheduler, or pipeline files to the 🤗 Hugging Face Hub.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your model, scheduler, or pipeline files to. It should\n",
      " |              contain your organization name when pushing to an organization. `repo_id` can also be a path to a local\n",
      " |              directory.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Default to `\"Upload {object}\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private.\n",
      " |          token (`str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. The token generated when running\n",
      " |              `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights to the `safetensors` format.\n",
      " |          variant (`str`, *optional*):\n",
      " |              If specified, weights are saved in the format `pytorch_model.<variant>.bin`.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from diffusers import UNet2DConditionModel\n",
      " |      \n",
      " |      unet = UNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2\", subfolder=\"unet\")\n",
      " |      \n",
      " |      # Push the `unet` to your namespace with the name \"my-finetuned-unet\".\n",
      " |      unet.push_to_hub(\"my-finetuned-unet\")\n",
      " |      \n",
      " |      # Push the `unet` to an organization with the name \"my-finetuned-unet\".\n",
      " |      unet.push_to_hub(\"your-org/my-finetuned-unet\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.loaders.TextualInversionLoaderMixin:\n",
      " |  \n",
      " |  load_textual_inversion(self, pretrained_model_name_or_path: Union[str, List[str], Dict[str, torch.Tensor], List[Dict[str, torch.Tensor]]], token: Union[str, List[str], NoneType] = None, tokenizer: Optional[ForwardRef('PreTrainedTokenizer')] = None, text_encoder: Optional[ForwardRef('PreTrainedModel')] = None, **kwargs)\n",
      " |      Load textual inversion embeddings into the text encoder of [`StableDiffusionPipeline`] (both 🤗 Diffusers and\n",
      " |      Automatic1111 formats are supported).\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike` or `List[str or os.PathLike]` or `Dict` or `List[Dict]`):\n",
      " |              Can be either one of the following or a list of them:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `sd-concepts-library/low-poly-hd-logos-icons`) of a\n",
      " |                    pretrained model hosted on the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_text_inversion_directory/`) containing the textual\n",
      " |                    inversion weights.\n",
      " |                  - A path to a *file* (for example `./my_text_inversions.pt`) containing textual inversion weights.\n",
      " |                  - A [torch state\n",
      " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
      " |      \n",
      " |          token (`str` or `List[str]`, *optional*):\n",
      " |              Override the token to use for the textual inversion weights. If `pretrained_model_name_or_path` is a\n",
      " |              list, then `token` must also be a list of equal length.\n",
      " |          text_encoder ([`~transformers.CLIPTextModel`], *optional*):\n",
      " |              Frozen text-encoder ([clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)).\n",
      " |              If not specified, function will take self.tokenizer.\n",
      " |          tokenizer ([`~transformers.CLIPTokenizer`], *optional*):\n",
      " |              A `CLIPTokenizer` to tokenize text. If not specified, function will take self.tokenizer.\n",
      " |          weight_name (`str`, *optional*):\n",
      " |              Name of a custom weight file. This should be used when:\n",
      " |      \n",
      " |                  - The saved textual inversion file is in 🤗 Diffusers format, but was saved under a specific weight\n",
      " |                    name such as `text_inv.bin`.\n",
      " |                  - The saved textual inversion file is in the Automatic1111 format.\n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      To load a textual inversion embedding vector in 🤗 Diffusers format:\n",
      " |      \n",
      " |      ```py\n",
      " |      from diffusers import StableDiffusionPipeline\n",
      " |      import torch\n",
      " |      \n",
      " |      model_id = \"runwayml/stable-diffusion-v1-5\"\n",
      " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
      " |      \n",
      " |      pipe.load_textual_inversion(\"sd-concepts-library/cat-toy\")\n",
      " |      \n",
      " |      prompt = \"A <cat-toy> backpack\"\n",
      " |      \n",
      " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
      " |      image.save(\"cat-backpack.png\")\n",
      " |      ```\n",
      " |      \n",
      " |      To load a textual inversion embedding vector in Automatic1111 format, make sure to download the vector first\n",
      " |      (for example from [civitAI](https://civitai.com/models/3036?modelVersionId=9857)) and then load the vector\n",
      " |      locally:\n",
      " |      \n",
      " |      ```py\n",
      " |      from diffusers import StableDiffusionPipeline\n",
      " |      import torch\n",
      " |      \n",
      " |      model_id = \"runwayml/stable-diffusion-v1-5\"\n",
      " |      pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
      " |      \n",
      " |      pipe.load_textual_inversion(\"./charturnerv2.pt\", token=\"charturnerv2\")\n",
      " |      \n",
      " |      prompt = \"charturnerv2, multiple views of the same character in the same outfit, a character turnaround of a woman wearing a black jacket and red shirt, best quality, intricate details.\"\n",
      " |      \n",
      " |      image = pipe(prompt, num_inference_steps=50).images[0]\n",
      " |      image.save(\"character.png\")\n",
      " |      ```\n",
      " |  \n",
      " |  maybe_convert_prompt(self, prompt: Union[str, List[str]], tokenizer: 'PreTrainedTokenizer')\n",
      " |      Processes prompts that include a special token corresponding to a multi-vector textual inversion embedding to\n",
      " |      be replaced with multiple special tokens each corresponding to one of the vectors. If the prompt has no textual\n",
      " |      inversion token or if the textual inversion token is a single vector, the input prompt is returned.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          prompt (`str` or list of `str`):\n",
      " |              The prompt or prompts to guide the image generation.\n",
      " |          tokenizer (`PreTrainedTokenizer`):\n",
      " |              The tokenizer responsible for encoding the prompt into input tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str` or list of `str`: The converted prompt\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  disable_lora(self)\n",
      " |  \n",
      " |  disable_lora_for_text_encoder(self, text_encoder: Optional[ForwardRef('PreTrainedModel')] = None)\n",
      " |      Disables the LoRA layers for the text encoder.\n",
      " |      \n",
      " |      Args:\n",
      " |          text_encoder (`torch.nn.Module`, *optional*):\n",
      " |              The text encoder module to disable the LoRA layers for. If `None`, it will try to get the\n",
      " |              `text_encoder` attribute.\n",
      " |  \n",
      " |  enable_lora(self)\n",
      " |  \n",
      " |  enable_lora_for_text_encoder(self, text_encoder: Optional[ForwardRef('PreTrainedModel')] = None)\n",
      " |      Enables the LoRA layers for the text encoder.\n",
      " |      \n",
      " |      Args:\n",
      " |          text_encoder (`torch.nn.Module`, *optional*):\n",
      " |              The text encoder module to enable the LoRA layers for. If `None`, it will try to get the `text_encoder`\n",
      " |              attribute.\n",
      " |  \n",
      " |  fuse_lora(self, fuse_unet: bool = True, fuse_text_encoder: bool = True, lora_scale: float = 1.0, safe_fusing: bool = False)\n",
      " |      Fuses the LoRA parameters into the original parameters of the corresponding blocks.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This is an experimental API.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          fuse_unet (`bool`, defaults to `True`): Whether to fuse the UNet LoRA parameters.\n",
      " |          fuse_text_encoder (`bool`, defaults to `True`):\n",
      " |              Whether to fuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the\n",
      " |              LoRA parameters then it won't have any effect.\n",
      " |          lora_scale (`float`, defaults to 1.0):\n",
      " |              Controls how much to influence the outputs with the LoRA parameters.\n",
      " |          safe_fusing (`bool`, defaults to `False`):\n",
      " |              Whether to check fused weights for NaN values before fusing and if values are NaN not fusing them.\n",
      " |  \n",
      " |  get_active_adapters(self) -> List[str]\n",
      " |      Gets the list of the current active adapters.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      from diffusers import DiffusionPipeline\n",
      " |      \n",
      " |      pipeline = DiffusionPipeline.from_pretrained(\n",
      " |          \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
      " |      ).to(\"cuda\")\n",
      " |      pipeline.load_lora_weights(\"CiroN2022/toy-face\", weight_name=\"toy_face_sdxl.safetensors\", adapter_name=\"toy\")\n",
      " |      pipeline.get_active_adapters()\n",
      " |      ```\n",
      " |  \n",
      " |  get_list_adapters(self) -> Dict[str, List[str]]\n",
      " |      Gets the current list of all available adapters in the pipeline.\n",
      " |  \n",
      " |  load_lora_weights(self, pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], adapter_name=None, **kwargs)\n",
      " |      Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.unet` and\n",
      " |      `self.text_encoder`.\n",
      " |      \n",
      " |      All kwargs are forwarded to `self.lora_state_dict`.\n",
      " |      \n",
      " |      See [`~loaders.LoraLoaderMixin.lora_state_dict`] for more details on how the state dict is loaded.\n",
      " |      \n",
      " |      See [`~loaders.LoraLoaderMixin.load_lora_into_unet`] for more details on how the state dict is loaded into\n",
      " |      `self.unet`.\n",
      " |      \n",
      " |      See [`~loaders.LoraLoaderMixin.load_lora_into_text_encoder`] for more details on how the state dict is loaded\n",
      " |      into `self.text_encoder`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n",
      " |              See [`~loaders.LoraLoaderMixin.lora_state_dict`].\n",
      " |          kwargs (`dict`, *optional*):\n",
      " |              See [`~loaders.LoraLoaderMixin.lora_state_dict`].\n",
      " |          adapter_name (`str`, *optional*):\n",
      " |              Adapter name to be used for referencing the loaded adapter model. If not specified, it will use\n",
      " |              `default_{i}` where i is the total number of adapters being loaded.\n",
      " |  \n",
      " |  set_adapters(self, adapter_names: Union[List[str], str], adapter_weights: Optional[List[float]] = None)\n",
      " |  \n",
      " |  set_adapters_for_text_encoder(self, adapter_names: Union[List[str], str], text_encoder: Optional[ForwardRef('PreTrainedModel')] = None, text_encoder_weights: List[float] = None)\n",
      " |      Sets the adapter layers for the text encoder.\n",
      " |      \n",
      " |      Args:\n",
      " |          adapter_names (`List[str]` or `str`):\n",
      " |              The names of the adapters to use.\n",
      " |          text_encoder (`torch.nn.Module`, *optional*):\n",
      " |              The text encoder module to set the adapter layers for. If `None`, it will try to get the `text_encoder`\n",
      " |              attribute.\n",
      " |          text_encoder_weights (`List[float]`, *optional*):\n",
      " |              The weights to use for the text encoder. If `None`, the weights are set to `1.0` for all the adapters.\n",
      " |  \n",
      " |  set_lora_device(self, adapter_names: List[str], device: Union[torch.device, str, int]) -> None\n",
      " |      Moves the LoRAs listed in `adapter_names` to a target device. Useful for offloading the LoRA to the CPU in case\n",
      " |      you want to load multiple adapters and free some GPU memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          adapter_names (`List[str]`):\n",
      " |              List of adapters to send device to.\n",
      " |          device (`Union[torch.device, str, int]`):\n",
      " |              Device to send the adapters to. Can be either a torch device, a str or an integer.\n",
      " |  \n",
      " |  unfuse_lora(self, unfuse_unet: bool = True, unfuse_text_encoder: bool = True)\n",
      " |      Reverses the effect of\n",
      " |      [`pipe.fuse_lora()`](https://huggingface.co/docs/diffusers/main/en/api/loaders#diffusers.loaders.LoraLoaderMixin.fuse_lora).\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This is an experimental API.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          unfuse_unet (`bool`, defaults to `True`): Whether to unfuse the UNet LoRA parameters.\n",
      " |          unfuse_text_encoder (`bool`, defaults to `True`):\n",
      " |              Whether to unfuse the text encoder LoRA parameters. If the text encoder wasn't monkey-patched with the\n",
      " |              LoRA parameters then it won't have any effect.\n",
      " |  \n",
      " |  unload_lora_weights(self)\n",
      " |      Unloads the LoRA parameters.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> # Assuming `pipeline` is already loaded with the LoRA parameters.\n",
      " |      >>> pipeline.unload_lora_weights()\n",
      " |      >>> ...\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  load_lora_into_text_encoder(state_dict, network_alphas, text_encoder, prefix=None, lora_scale=1.0, low_cpu_mem_usage=None, adapter_name=None, _pipeline=None) from builtins.type\n",
      " |      This will load the LoRA layers specified in `state_dict` into `text_encoder`\n",
      " |      \n",
      " |      Parameters:\n",
      " |          state_dict (`dict`):\n",
      " |              A standard state dict containing the lora layer parameters. The key should be prefixed with an\n",
      " |              additional `text_encoder` to distinguish between unet lora layers.\n",
      " |          network_alphas (`Dict[str, float]`):\n",
      " |              See `LoRALinearLayer` for more details.\n",
      " |          text_encoder (`CLIPTextModel`):\n",
      " |              The text encoder model to load the LoRA layers into.\n",
      " |          prefix (`str`):\n",
      " |              Expected prefix of the `text_encoder` in the `state_dict`.\n",
      " |          lora_scale (`float`):\n",
      " |              How much to scale the output of the lora linear layer before it is added with the output of the regular\n",
      " |              lora layer.\n",
      " |          low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
      " |              Speed up model loading only loading the pretrained weights and not initializing the weights. This also\n",
      " |              tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      " |              Only supported for PyTorch >= 1.9.0. If you are using an older version of PyTorch, setting this\n",
      " |              argument to `True` will raise an error.\n",
      " |          adapter_name (`str`, *optional*):\n",
      " |              Adapter name to be used for referencing the loaded adapter model. If not specified, it will use\n",
      " |              `default_{i}` where i is the total number of adapters being loaded.\n",
      " |  \n",
      " |  load_lora_into_unet(state_dict, network_alphas, unet, low_cpu_mem_usage=None, adapter_name=None, _pipeline=None) from builtins.type\n",
      " |      This will load the LoRA layers specified in `state_dict` into `unet`.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          state_dict (`dict`):\n",
      " |              A standard state dict containing the lora layer parameters. The keys can either be indexed directly\n",
      " |              into the unet or prefixed with an additional `unet` which can be used to distinguish between text\n",
      " |              encoder lora layers.\n",
      " |          network_alphas (`Dict[str, float]`):\n",
      " |              See `LoRALinearLayer` for more details.\n",
      " |          unet (`UNet2DConditionModel`):\n",
      " |              The UNet model to load the LoRA layers into.\n",
      " |          low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
      " |              Speed up model loading only loading the pretrained weights and not initializing the weights. This also\n",
      " |              tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      " |              Only supported for PyTorch >= 1.9.0. If you are using an older version of PyTorch, setting this\n",
      " |              argument to `True` will raise an error.\n",
      " |          adapter_name (`str`, *optional*):\n",
      " |              Adapter name to be used for referencing the loaded adapter model. If not specified, it will use\n",
      " |              `default_{i}` where i is the total number of adapters being loaded.\n",
      " |  \n",
      " |  lora_state_dict(pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]], **kwargs) from builtins.type\n",
      " |      Return state dict for lora weights and the network alphas.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      We support loading A1111 formatted LoRA checkpoints in a limited capacity.\n",
      " |      \n",
      " |      This function is experimental and might change in the future.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |                  - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on\n",
      " |                    the Hub.\n",
      " |                  - A path to a *directory* (for example `./my_model_directory`) containing the model weights saved\n",
      " |                    with [`ModelMixin.save_pretrained`].\n",
      " |                  - A [torch state\n",
      " |                    dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n",
      " |      \n",
      " |          cache_dir (`Union[str, os.PathLike]`, *optional*):\n",
      " |              Path to a directory where a downloaded pretrained model configuration is cached if the standard cache\n",
      " |              is not used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      " |              cached versions if they exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to resume downloading the model weights and configuration files. If set to `False`, any\n",
      " |              incompletely downloaded files are deleted.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to only load local model weights and configuration files or not. If set to `True`, the model\n",
      " |              won't be downloaded from the Hub.\n",
      " |          use_auth_token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from\n",
      " |              `diffusers-cli login` (stored in `~/.huggingface`) is used.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier\n",
      " |              allowed by Git.\n",
      " |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      " |              The subfolder location of a model file within a larger model repository on the Hub or locally.\n",
      " |          low_cpu_mem_usage (`bool`, *optional*, defaults to `True` if torch version >= 1.9.0 else `False`):\n",
      " |              Speed up model loading only loading the pretrained weights and not initializing the weights. This also\n",
      " |              tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model.\n",
      " |              Only supported for PyTorch >= 1.9.0. If you are using an older version of PyTorch, setting this\n",
      " |              argument to `True` will raise an error.\n",
      " |          mirror (`str`, *optional*):\n",
      " |              Mirror source to resolve accessibility issues if you're downloading a model in China. We do not\n",
      " |              guarantee the timeliness or safety of the source, and you should refer to the mirror site for more\n",
      " |              information.\n",
      " |  \n",
      " |  save_lora_weights(save_directory: Union[str, os.PathLike], unet_lora_layers: Dict[str, Union[torch.nn.modules.module.Module, torch.Tensor]] = None, text_encoder_lora_layers: Dict[str, torch.nn.modules.module.Module] = None, is_main_process: bool = True, weight_name: str = None, save_function: Callable = None, safe_serialization: bool = True) from builtins.type\n",
      " |      Save the LoRA parameters corresponding to the UNet and text encoder.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              Directory to save LoRA parameters to. Will be created if it doesn't exist.\n",
      " |          unet_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n",
      " |              State dict of the LoRA layers corresponding to the `unet`.\n",
      " |          text_encoder_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n",
      " |              State dict of the LoRA layers corresponding to the `text_encoder`. Must explicitly pass the text\n",
      " |              encoder LoRA state dict because it comes from 🤗 Transformers.\n",
      " |          is_main_process (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the process calling this is the main process or not. Useful during distributed training and you\n",
      " |              need to call this function on all processes. In this case, set `is_main_process=True` only on the main\n",
      " |              process to avoid race conditions.\n",
      " |          save_function (`Callable`):\n",
      " |              The function to use to save the state dictionary. Useful during distributed training when you need to\n",
      " |              replace `torch.save` with another method. Can be configured with the environment variable\n",
      " |              `DIFFUSERS_SAVE_MODE`.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether to save the model using `safetensors` or the traditional PyTorch way with `pickle`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  write_lora_layers(state_dict: Dict[str, torch.Tensor], save_directory: str, is_main_process: bool, weight_name: str, save_function: Callable, safe_serialization: bool)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  lora_scale\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from diffusers.loaders.LoraLoaderMixin:\n",
      " |  \n",
      " |  num_fused_loras = 0\n",
      " |  \n",
      " |  text_encoder_name = 'text_encoder'\n",
      " |  \n",
      " |  unet_name = 'unet'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea113ab-9d99-4e67-ab84-9ce38a50317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Transformers library is installed at: C:\\Users\\wn00217454\\Anaconda3\\envs\\cv\\lib\\site-packages/transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import site\n",
    "\n",
    "# Get the site-packages directory paths where Python libraries are installed\n",
    "site_packages_paths = site.getsitepackages()\n",
    "\n",
    "# Search for the Transformers library path within site-packages directories\n",
    "transformers_path = None\n",
    "for path in site_packages_paths:\n",
    "    potential_transformers_path = path + \"/transformers\"\n",
    "    if os.path.exists(potential_transformers_path):\n",
    "        transformers_path = potential_transformers_path\n",
    "        break\n",
    "\n",
    "if transformers_path:\n",
    "    print(\"Hugging Face Transformers library is installed at:\", transformers_path)\n",
    "else:\n",
    "    print(\"Hugging Face Transformers library is not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef39f8a-763a-498d-ba98-4117be3dffa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wn00217454\\Anaconda3\\envs\\cv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'hf_bucket_url' from 'transformers.file_utils' (C:\\Users\\wn00217454\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\file_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_bucket_url\n\u001b[0;32m      3\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m hf_bucket_url()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(cache_dir)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'hf_bucket_url' from 'transformers.file_utils' (C:\\Users\\wn00217454\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\file_utils.py)"
     ]
    }
   ],
   "source": [
    "from transformers.file_utils import hf_bucket_url\n",
    "\n",
    "cache_dir = hf_bucket_url()\n",
    "print(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b77f19e9-a665-4b53-88df-32619d5022eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'hf_bucket_url' from 'transformers' (C:\\Users\\wn00217454\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_bucket_url, cachedir\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cachedir)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'hf_bucket_url' from 'transformers' (C:\\Users\\wn00217454\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import hf_bucket_url, cachedir\n",
    "\n",
    "print(cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7120da2-bdf4-4e40-b8f5-4918bb35f2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
